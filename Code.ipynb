{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkbDenV2Tn9M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import spacy\n",
        "import networkx as nx\n",
        "import dataclasses\n",
        "import gc\n",
        "import io\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
        "import pathway as pw\n",
        "\n",
        "\n",
        "#Configuration\n",
        "\n",
        "class GlobalConfig:\n",
        "    # Google Drive IDs for pathway configuration\n",
        "\n",
        "    TRAIN_CSV_GDRIVE_ID = OBJECT_ID OF YOUR TRAIN.CSV FILE\n",
        "    TEST_CSV_GDRIVE_ID = OBJECT_ID OF YOUR TEST.CSV FILE\n",
        "    BOOKS_DIR_ID = OBJECT_ID OF THE BOOKS DIRECTORY\n",
        "    CREDENTIALS_PATH = PATH OF YOUR GOOGLE CLOUD CREDETIALS JSON\n",
        "\n",
        "\n",
        "    TRAIN_PKL = \"processed_train_data_pretrained.pkl\"\n",
        "    SUBMISSION_FILE = \"Results.csv\"\n",
        "\n",
        "    # Model Hyperparameters\n",
        "    N_FOLDS = 5\n",
        "    BATCH_SIZE = 4\n",
        "    ACCUM_STEPS = 4\n",
        "    EPOCHS = 25\n",
        "    LR = 1e-4\n",
        "    SEED = 42\n",
        "\n",
        "    # Feature Config\n",
        "    CHUNK_SIZE = 1000\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class ModelConfig:\n",
        "    n_embd: int = 128          # BDH embedding dimension\n",
        "    n_head: int = 4            # Attention heads\n",
        "    n_feat: int = 775          # 770 pretrained + 3 graph + 2 NLI\n",
        "    seq_len: int = 16          # Sequence length for hypothesis tokens\n",
        "    dropout: float = 0.3\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "# pathway data loading\n",
        "\n",
        "def load_pathway_data():\n",
        "    \"\"\"Loads Train, Test, and Books from Google Drive via Pathway.\"\"\"\n",
        "\n",
        "    # Load Train CSV\n",
        "    print(\"Loading train.csv...\")\n",
        "    try:\n",
        "        train_table = pw.io.gdrive.read(\n",
        "            object_id=GlobalConfig.TRAIN_CSV_GDRIVE_ID,\n",
        "            service_user_credentials_file=GlobalConfig.CREDENTIALS_PATH,\n",
        "            mode=\"static\",\n",
        "            format=\"binary\"\n",
        "        )\n",
        "        train_result = pw.debug.table_to_pandas(train_table)\n",
        "        csv_content = train_result['data'].iloc[0].decode('utf-8')\n",
        "        train_df = pd.read_csv(io.StringIO(csv_content))\n",
        "        print(f\"Train Loaded: {len(train_df)} rows\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Train: {e}\")\n",
        "        train_df = pd.DataFrame()\n",
        "\n",
        "    # Load Test CSV\n",
        "    print(\"Loading test.csv...\")\n",
        "    try:\n",
        "        test_table = pw.io.gdrive.read(\n",
        "            object_id=GlobalConfig.TEST_CSV_GDRIVE_ID,\n",
        "            service_user_credentials_file=GlobalConfig.CREDENTIALS_PATH,\n",
        "            mode=\"static\",\n",
        "            format=\"binary\"\n",
        "        )\n",
        "        test_result = pw.debug.table_to_pandas(test_table)\n",
        "        csv_content = test_result['data'].iloc[0].decode('utf-8')\n",
        "        test_df = pd.read_csv(io.StringIO(csv_content))\n",
        "        print(f\"Test Loaded: {len(test_df)} rows\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Test: {e}\")\n",
        "        test_df = pd.DataFrame()\n",
        "\n",
        "    # Load Books Directory\n",
        "    print(\"Loading Books Directory...\")\n",
        "    try:\n",
        "        books_table = pw.io.gdrive.read(\n",
        "            object_id=GlobalConfig.BOOKS_DIR_ID,\n",
        "            service_user_credentials_file=GlobalConfig.CREDENTIALS_PATH,\n",
        "            mode=\"static\",\n",
        "            format=\"binary\",\n",
        "            with_metadata=True\n",
        "        )\n",
        "        books_df = pw.debug.table_to_pandas(books_table)\n",
        "        print(f\"Books Loaded: {len(books_df)} files\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Books: {e}\")\n",
        "        books_df = pd.DataFrame()\n",
        "\n",
        "    return train_df, test_df, books_df\n",
        "\n",
        "\n",
        "#helper function for normalizing and finding books\n",
        "\n",
        "def normalize_name(name):\n",
        "    \"\"\"Normalize book names for matching\"\"\"\n",
        "    if not name: return \"\"\n",
        "    return str(name).lower().replace(\".txt\", \"\").replace(\" \", \"\").replace('\"', '').replace(\"'\", \"\")\n",
        "\n",
        "def find_book_content_in_df(books_df, book_name):\n",
        "    \"\"\"\n",
        "    Finds a book in the Pathway DataFrame and returns its text content.\n",
        "    Returns: text_content (str) or None\n",
        "    \"\"\"\n",
        "    if books_df.empty: return None\n",
        "\n",
        "    target_clean = normalize_name(book_name)\n",
        "\n",
        "    # Iterate to find match (exact match on normalized name)\n",
        "    for idx, row in books_df.iterrows():\n",
        "        file_raw = str(row['_metadata']['name'])\n",
        "        file_clean = normalize_name(file_raw)\n",
        "        if file_clean == target_clean:\n",
        "            try:\n",
        "                return row['data'].decode('utf-8', errors='ignore')\n",
        "            except:\n",
        "                return None\n",
        "    return None\n",
        "\n",
        "\n",
        "# Feature engineering and extraction\n",
        "\n",
        "class UnifiedFeatureEngine:\n",
        "    \"\"\"\n",
        "    Handles feature extraction combining Geometric (Embeddings), Graph, and NLI features.\n",
        "    \"\"\"\n",
        "    def __init__(self, train_mean=None, train_std=None):\n",
        "        print(\"Loading Feature Models...\")\n",
        "\n",
        "        # Geometric Encoder (Sentence Transformers)\n",
        "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "        self.retriever = self.encoder\n",
        "        print(\"Loaded sentence encoder (384-dim)\")\n",
        "\n",
        "        #  NLI Model\n",
        "        self.nli = CrossEncoder('cross-encoder/nli-deberta-v3-base', device=device)\n",
        "        print(\"Loaded NLI cross-encoder\")\n",
        "\n",
        "        # Graph/Spacy\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except:\n",
        "            print(\"Downloading spacy model...\")\n",
        "            os.system(\"python -m spacy download en_core_web_sm\")\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "        # Normalization stats (for inference)\n",
        "        self.train_mean = train_mean\n",
        "        self.train_std = train_std\n",
        "        print(\"Feature Engine Ready\")\n",
        "\n",
        "    def get_diverse_chunks(self, book_text):\n",
        "        \"\"\"Sample chunks from beginning, middle, and end.\"\"\"\n",
        "        chunk_size = GlobalConfig.CHUNK_SIZE\n",
        "        book_len = len(book_text)\n",
        "        if book_len < chunk_size:\n",
        "            return [book_text]\n",
        "\n",
        "        chunks = []\n",
        "        # Beginning chunk\n",
        "        chunks.extend([book_text[i:i+chunk_size] for i in range(0, int(book_len * 0.2), chunk_size)])\n",
        "        # Middle chunk\n",
        "        chunks.extend([book_text[i:i+chunk_size] for i in range(int(book_len * 0.4), int(book_len * 0.6), chunk_size)])\n",
        "        # End chunk\n",
        "        chunks.extend([book_text[i:i+chunk_size] for i in range(int(book_len * 0.8), book_len, chunk_size)])\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def get_graph_features(self, text):\n",
        "        \"\"\"Extract character interaction graph features (3 dims).\"\"\"\n",
        "        G = nx.Graph()\n",
        "        doc = self.nlp(text[:30000])\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            chars = [e.text for e in sent.ents if e.label_ == \"PERSON\"]\n",
        "            if len(chars) > 1:\n",
        "                for i in range(len(chars)):\n",
        "                    for j in range(i+1, len(chars)):\n",
        "                        G.add_edge(chars[i], chars[j])\n",
        "\n",
        "        if len(G) == 0:\n",
        "            return [0.0, 0.0, 0.0]\n",
        "\n",
        "        pr = list(nx.pagerank(G).values())\n",
        "        return [\n",
        "            np.mean(pr) if pr else 0,\n",
        "            nx.density(G),\n",
        "            len(G.edges) / max(1, len(G.nodes))\n",
        "        ]\n",
        "\n",
        "    def get_nli_features(self, backstory, book_text):\n",
        "        \"\"\"Semantic search + NLI (2 dims).\"\"\"\n",
        "        try:\n",
        "            if len(book_text) < 100: return [0.0, 0.0]\n",
        "\n",
        "            full_chunks = self.get_diverse_chunks(book_text)\n",
        "            if not full_chunks: return [0.0, 0.0]\n",
        "\n",
        "            # Semantic Search\n",
        "            query_emb = self.retriever.encode(backstory, convert_to_tensor=True)\n",
        "            chunk_embs = self.retriever.encode(full_chunks, convert_to_tensor=True, batch_size=32, show_progress_bar=False)\n",
        "\n",
        "            top_k = min(5, len(full_chunks))\n",
        "            hits = util.semantic_search(query_emb, chunk_embs, top_k=top_k)[0]\n",
        "            relevant_chunks = [full_chunks[hit['corpus_id']] for hit in hits]\n",
        "\n",
        "            # NLI\n",
        "            pairs = [(backstory, chunk) for chunk in relevant_chunks]\n",
        "            scores = self.nli.predict(pairs)\n",
        "            probs = torch.softmax(torch.tensor(scores), dim=1)\n",
        "\n",
        "            # Aggregate Top contradictions\n",
        "            contradiction_probs = probs[:, 0]\n",
        "            k_contradict = min(3, len(contradiction_probs))\n",
        "            topk_indices = contradiction_probs.topk(k_contradict).indices\n",
        "            nli_probs = probs[topk_indices].mean(dim=0).tolist()\n",
        "\n",
        "            return nli_probs[:2] # Contradiction, Entailment\n",
        "        except:\n",
        "            return [0.0, 0.0]\n",
        "\n",
        "    def get_pretrained_embeddings(self, backstory, book_text):\n",
        "\n",
        "        # Encode Backstory\n",
        "        backstory_emb = self.encoder.encode(backstory, convert_to_tensor=True, show_progress_bar=False)\n",
        "\n",
        "        # Encode Book (Beg/Mid/End)\n",
        "        book_len = len(book_text)\n",
        "        excerpts = [\n",
        "            book_text[:2000],\n",
        "            book_text[book_len//2-1000:book_len//2+1000],\n",
        "            book_text[-2000:] if book_len > 2000 else book_text\n",
        "        ]\n",
        "        book_embs = self.encoder.encode(excerpts, convert_to_tensor=True, show_progress_bar=False)\n",
        "        book_emb = book_embs.mean(dim=0)\n",
        "\n",
        "        # Interaction Features\n",
        "        cos_sim = torch.nn.functional.cosine_similarity(backstory_emb.unsqueeze(0), book_emb.unsqueeze(0)).item()\n",
        "        l2_dist = torch.norm(backstory_emb - book_emb).item()\n",
        "\n",
        "        combined = torch.cat([\n",
        "            backstory_emb.cpu(),\n",
        "            book_emb.cpu(),\n",
        "            torch.tensor([cos_sim, l2_dist])\n",
        "        ])\n",
        "        return combined.numpy()\n",
        "\n",
        "    def extract_all(self, backstory, book_text, normalize=False):\n",
        "        \"\"\"Master method to get the 775-dim vector.\"\"\"\n",
        "        emb_feats = self.get_pretrained_embeddings(backstory, book_text)\n",
        "        graph_feats = self.get_graph_features(book_text)\n",
        "        nli_feats = self.get_nli_features(backstory, book_text)\n",
        "\n",
        "        all_features = np.concatenate([\n",
        "            emb_feats,\n",
        "            np.array(graph_feats),\n",
        "            np.array(nli_feats)\n",
        "        ])\n",
        "\n",
        "        # Apply normalization if inference\n",
        "        if normalize and self.train_mean is not None:\n",
        "            all_features = (all_features - self.train_mean) / self.train_std\n",
        "\n",
        "        return {\n",
        "            'features': all_features,\n",
        "            'nli_raw': nli_feats\n",
        "        }\n",
        "\n",
        "# BDH Model Architecture\n",
        "\n",
        "class GraphFusion(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.feat_proj = nn.Linear(config.n_feat, config.n_embd)\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(config.n_embd * 2, config.n_embd),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, features):\n",
        "        f = self.feat_proj(features).unsqueeze(1).expand(-1, x.size(1), -1)\n",
        "        combined = torch.cat([x, f], dim=-1)\n",
        "        gate = self.gate(combined)\n",
        "        fused = x * (1 - gate) + f * gate\n",
        "        return self.dropout(fused)\n",
        "\n",
        "class BabyDragonHatchling(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.feature_encoder = nn.Sequential(\n",
        "            nn.Linear(config.n_feat, config.n_embd * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout),\n",
        "            nn.Linear(config.n_embd * 2, config.n_embd)\n",
        "        )\n",
        "        self.hypothesis_tokens = nn.Parameter(torch.randn(1, config.seq_len, config.n_embd) * 0.02)\n",
        "        self.fusion = GraphFusion(config)\n",
        "        self.encoder = nn.TransformerEncoderLayer(\n",
        "            d_model=config.n_embd, nhead=config.n_head,\n",
        "            dim_feedforward=config.n_embd * 4, dropout=config.dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.n_embd))\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, config.n_embd // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout),\n",
        "            nn.Linear(config.n_embd // 2, 1)\n",
        "        )\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=0.5)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, features):\n",
        "        B = features.size(0)\n",
        "        h_tokens = self.hypothesis_tokens.expand(B, -1, -1)\n",
        "        x = self.fusion(h_tokens, features)\n",
        "        cls = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls, x], dim=1)\n",
        "        x = self.encoder(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "\n",
        "# Dataset and training Logic\n",
        "class PretrainedDataset(Dataset):\n",
        "    def __init__(self, data_list, mean=None, std=None):\n",
        "        self.data = data_list\n",
        "        feats = np.array([d['features'] for d in self.data])\n",
        "        self.mean = feats.mean(axis=0) if mean is None else mean\n",
        "        self.std = (feats.std(axis=0) + 1e-6) if std is None else std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        norm_feat = (np.array(item['features']) - self.mean) / self.std\n",
        "        label = item.get('label', 0.0)\n",
        "        if torch.is_tensor(label): label = label.item()\n",
        "\n",
        "        return {\n",
        "            'features': torch.tensor(norm_feat, dtype=torch.float32),\n",
        "            'label': torch.tensor(label, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "def train_one_fold(fold_idx, train_ds, val_ds, config):\n",
        "\n",
        "    labels = [d['label'].item() for d in train_ds]\n",
        "    counts = [labels.count(0), labels.count(1)]\n",
        "    if counts[0] == 0: counts[0] = 1\n",
        "    if counts[1] == 0: counts[1] = 1\n",
        "\n",
        "    weights = [1.0/counts[int(l)] for l in labels]\n",
        "    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=GlobalConfig.BATCH_SIZE, sampler=sampler)\n",
        "    val_dl = DataLoader(val_ds, batch_size=GlobalConfig.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = BabyDragonHatchling(config).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=GlobalConfig.LR, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "\n",
        "    pos_weight = torch.tensor([counts[0] / max(1, counts[1])]).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_thresh = 0.5\n",
        "    patience = 8\n",
        "    counter = 0\n",
        "\n",
        "    print(f\"\\n Goal: Fold {fold_idx+1} Training...\")\n",
        "\n",
        "    for epoch in range(GlobalConfig.EPOCHS):\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_dl):\n",
        "            loss = criterion(model(batch['features'].to(device)), batch['label'].to(device).unsqueeze(1))\n",
        "            loss = loss / GlobalConfig.ACCUM_STEPS\n",
        "            loss.backward()\n",
        "            if (i+1) % GlobalConfig.ACCUM_STEPS == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        probs, trues = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dl:\n",
        "                logits = model(batch['features'].to(device))\n",
        "                probs.extend(torch.sigmoid(logits).cpu().numpy().flatten())\n",
        "                trues.extend(batch['label'].cpu().numpy().flatten())\n",
        "\n",
        "        # Threshold Search\n",
        "        curr_best_f1 = 0\n",
        "        curr_thresh = 0.5\n",
        "        for t in np.linspace(0.2, 0.8, 61):\n",
        "            f1 = f1_score(trues, (np.array(probs) > t).astype(int), zero_division=0)\n",
        "            if f1 > curr_best_f1: curr_best_f1, curr_thresh = f1, t\n",
        "\n",
        "        scheduler.step(curr_best_f1)\n",
        "\n",
        "        if curr_best_f1 >= best_f1:\n",
        "            best_f1, best_thresh = curr_best_f1, curr_thresh\n",
        "            counter = 0\n",
        "            torch.save({\n",
        "                'model': model.state_dict(),\n",
        "                'threshold': best_thresh,\n",
        "                'config': config\n",
        "            }, f\"bdh_fold_{fold_idx}_best.pth\")\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience: break\n",
        "\n",
        "    return best_f1\n",
        "\n",
        "\n",
        "# Orchestration of Pipeline\n",
        "def run_preprocessing(train_df, books_df):\n",
        "    \"\"\"Step 1: Convert raw books/backstories into feature vectors.\"\"\"\n",
        "\n",
        "\n",
        "    engine = UnifiedFeatureEngine()\n",
        "    processed_data = []\n",
        "\n",
        "    for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Extracting Features\"):\n",
        "        book_name = row['book_name']\n",
        "\n",
        "        # Find book content in Pathway DataFrame\n",
        "        text = find_book_content_in_df(books_df, book_name)\n",
        "        if not text:\n",
        "            print(f\"Missing book: {book_name}\")\n",
        "            continue\n",
        "\n",
        "        # Label parsing\n",
        "        lbl_str = str(row.get('label', 1)).lower()\n",
        "        label_val = 0.0 if 'contra' in lbl_str else 1.0\n",
        "\n",
        "        # Extract features (Unified 775-dim)\n",
        "        feats = engine.extract_all(row.get('content', ''), text, normalize=False)\n",
        "        feats['label'] = label_val\n",
        "        processed_data.append(feats)\n",
        "\n",
        "    with open(GlobalConfig.TRAIN_PKL, 'wb') as f:\n",
        "        pickle.dump(processed_data, f)\n",
        "    print(f\"Saved {len(processed_data)} samples to {GlobalConfig.TRAIN_PKL}\")\n",
        "\n",
        "def run_training():\n",
        "    if not os.path.exists(GlobalConfig.TRAIN_PKL):\n",
        "        print(\"Data not found. Run preprocessing first.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n Training BDH\")\n",
        "\n",
        "    with open(GlobalConfig.TRAIN_PKL, 'rb') as f:\n",
        "        raw_data = pickle.load(f)\n",
        "\n",
        "    if len(raw_data) == 0:\n",
        "        print(\"No training data available.\")\n",
        "        return\n",
        "\n",
        "    # Init main dataset to get stats\n",
        "    full_ds = PretrainedDataset(raw_data)\n",
        "    labels = [d['label'] for d in raw_data]\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=GlobalConfig.N_FOLDS, shuffle=True, random_state=GlobalConfig.SEED)\n",
        "    scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(raw_data, labels)):\n",
        "        train_sub = [raw_data[i] for i in train_idx]\n",
        "        val_sub = [raw_data[i] for i in val_idx]\n",
        "\n",
        "        # Create datasets using global mean/std from full_ds to ensure consistency\n",
        "        train_ds = PretrainedDataset(train_sub, mean=full_ds.mean, std=full_ds.std)\n",
        "        val_ds = PretrainedDataset(val_sub, mean=full_ds.mean, std=full_ds.std)\n",
        "\n",
        "        f1 = train_one_fold(fold, train_ds, val_ds, ModelConfig())\n",
        "        scores.append(f1)\n",
        "        print(f\"Fold {fold+1} Best F1: {f1:.2%}\")\n",
        "\n",
        "    print(f\"\\n Mean F1: {np.mean(scores):.2%} Â± {np.std(scores):.2%}\")\n",
        "\n",
        "\n",
        "# Generation of submission on test csv\n",
        "\n",
        "def run_inference(test_df, books_df):\n",
        "\n",
        "    print(\"\\n Inference\")\n",
        "\n",
        "    if test_df.empty:\n",
        "        print(\"Test DF is empty.\")\n",
        "        return\n",
        "\n",
        "    # Load stats from training data for normalization\n",
        "    try:\n",
        "        with open(GlobalConfig.TRAIN_PKL, 'rb') as f:\n",
        "            train_data = pickle.load(f)\n",
        "        train_feats = np.array([d['features'] for d in train_data])\n",
        "        mean, std = train_feats.mean(axis=0), train_feats.std(axis=0) + 1e-6\n",
        "    except:\n",
        "        print(\"Training data missing, calculating generic stats (not ideal)\")\n",
        "        mean, std = None, None\n",
        "\n",
        "    # Load Models\n",
        "    models, thresholds = [], []\n",
        "    for i in range(GlobalConfig.N_FOLDS):\n",
        "        path = f\"bdh_fold_{i}_best.pth\"\n",
        "        if os.path.exists(path):\n",
        "\n",
        "            ckpt = torch.load(path, map_location=device, weights_only=False)\n",
        "\n",
        "            m = BabyDragonHatchling(ckpt['config']).to(device)\n",
        "            m.load_state_dict(ckpt['model'])\n",
        "            m.eval()\n",
        "            models.append(m)\n",
        "            thresholds.append(ckpt.get('threshold', 0.5))\n",
        "\n",
        "    if not models:\n",
        "        print(\" No models found!\")\n",
        "        return\n",
        "\n",
        "    avg_thresh = np.mean(thresholds)\n",
        "    print(f\"Loaded {len(models)} models. Avg Threshold: {avg_thresh:.3f}\")\n",
        "\n",
        "    # Process\n",
        "    engine = UnifiedFeatureEngine(train_mean=mean, train_std=std)\n",
        "    results = []\n",
        "    stats = {'consistent': 0, 'contradict': 0}\n",
        "\n",
        "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Inference\"):\n",
        "        row_id = row['id']\n",
        "        book_name = row['book_name']\n",
        "\n",
        "        # Find book content\n",
        "        text = find_book_content_in_df(books_df, book_name)\n",
        "\n",
        "        # Fail-safe defaults\n",
        "        if not text:\n",
        "            results.append({'id': row_id, 'label': 1, 'rationale': 'Book missing'})\n",
        "            stats['consistent'] += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Extract & Normalize\n",
        "            extracted = engine.extract_all(row.get('content', ''), text, normalize=True)\n",
        "            feat_tensor = torch.tensor(extracted['features'], dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "            # Ensemble Prediction\n",
        "            probs = []\n",
        "            with torch.no_grad():\n",
        "                for m in models:\n",
        "                    probs.append(torch.sigmoid(m(feat_tensor)).item())\n",
        "\n",
        "            avg_prob = np.mean(probs)\n",
        "            pred = 1 if avg_prob > avg_thresh else 0\n",
        "\n",
        "            # Rationale\n",
        "            rat = f\"Contradiction (Prob: {avg_prob:.2f})\" if pred == 0 else f\"Consistent (Prob: {avg_prob:.2f})\"\n",
        "            results.append({'id': row_id, 'label': pred, 'rationale': rat})\n",
        "\n",
        "            if pred == 1: stats['consistent'] += 1\n",
        "            else: stats['contradict'] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            results.append({'id': row_id, 'label': 1, 'rationale': str(e)})\n",
        "\n",
        "    pd.DataFrame(results).to_csv(GlobalConfig.SUBMISSION_FILE, index=False)\n",
        "    print(f\"\\n Stats: {stats}\")\n",
        "    print(f\"Submission saved to {GlobalConfig.SUBMISSION_FILE}\")\n",
        "\n",
        "# Main orchestrator\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    set_seed(GlobalConfig.SEED)\n",
        "\n",
        "    # Load Data from Drive via Pathway\n",
        "    train_df, test_df, books_df = load_pathway_data()\n",
        "\n",
        "    if not train_df.empty and not books_df.empty:\n",
        "        # Preprocess\n",
        "        run_preprocessing(train_df, books_df)\n",
        "\n",
        "        # Train\n",
        "        run_training()\n",
        "\n",
        "        # Inference\n",
        "        run_inference(test_df, books_df)\n",
        "    else:\n",
        "        print(\"Failed to load initial data. Check GDrive IDs.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCkcFYdkAYUI"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
